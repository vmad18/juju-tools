{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-28 17:34:03,779 - bitsandbytes.cextension - DEBUG -> Loading bitsandbytes native library from: c:\\Users\\volta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\volta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "from juju_tools.utils import *\n",
    "\n",
    "from juju_tools.models.loading_utils import ModelLoader\n",
    "from juju_tools.models.llama3 import Tokenizer, ChatFormat\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-28 17:34:05,879 - root - INFO ->  Model has been loaded... \n",
      "2024-06-28 17:34:05,880 - root - INFO -> Quantizing Model Layers... \n",
      "2024-06-28 17:34:05,881 - root - INFO -> ('layers', 'embed', 'cls_head', 'pre_norm')\n",
      "2024-06-28 17:34:05,881 - root - INFO -> ('gq_attn', 'ffn', 'attn_prenorm', 'ffn_prenorm')\n",
      "2024-06-28 17:34:05,881 - root - INFO -> Quantizing block: 0 \n",
      "2024-06-28 17:34:07,211 - root - INFO -> Quantizing block: 1 \n",
      "2024-06-28 17:34:08,582 - root - INFO -> Quantizing block: 2 \n",
      "2024-06-28 17:34:09,946 - root - INFO -> Quantizing block: 3 \n",
      "2024-06-28 17:34:11,218 - root - INFO -> Quantizing block: 4 \n",
      "2024-06-28 17:34:12,516 - root - INFO -> Quantizing block: 5 \n",
      "2024-06-28 17:34:13,750 - root - INFO -> Quantizing block: 6 \n",
      "2024-06-28 17:34:15,059 - root - INFO -> Quantizing block: 7 \n",
      "2024-06-28 17:34:16,375 - root - INFO -> Quantizing block: 8 \n",
      "2024-06-28 17:34:17,705 - root - INFO -> Quantizing block: 9 \n",
      "2024-06-28 17:34:18,980 - root - INFO -> Quantizing block: 10 \n",
      "2024-06-28 17:34:20,277 - root - INFO -> Quantizing block: 11 \n",
      "2024-06-28 17:34:21,586 - root - INFO -> Quantizing block: 12 \n",
      "2024-06-28 17:34:22,894 - root - INFO -> Quantizing block: 13 \n",
      "2024-06-28 17:34:24,190 - root - INFO -> Quantizing block: 14 \n",
      "2024-06-28 17:34:25,497 - root - INFO -> Quantizing block: 15 \n",
      "2024-06-28 17:34:26,730 - root - INFO -> Quantizing block: 16 \n",
      "2024-06-28 17:34:27,970 - root - INFO -> Quantizing block: 17 \n",
      "2024-06-28 17:34:29,206 - root - INFO -> Quantizing block: 18 \n",
      "2024-06-28 17:34:30,427 - root - INFO -> Quantizing block: 19 \n",
      "2024-06-28 17:34:31,649 - root - INFO -> Quantizing block: 20 \n",
      "2024-06-28 17:34:32,869 - root - INFO -> Quantizing block: 21 \n",
      "2024-06-28 17:34:34,085 - root - INFO -> Quantizing block: 22 \n",
      "2024-06-28 17:34:35,325 - root - INFO -> Quantizing block: 23 \n",
      "2024-06-28 17:34:36,609 - root - INFO -> Quantizing block: 24 \n",
      "2024-06-28 17:34:37,903 - root - INFO -> Quantizing block: 25 \n",
      "2024-06-28 17:34:39,230 - root - INFO -> Quantizing block: 26 \n",
      "2024-06-28 17:34:40,507 - root - INFO -> Quantizing block: 27 \n",
      "2024-06-28 17:34:41,781 - root - INFO -> Quantizing block: 28 \n",
      "2024-06-28 17:34:43,105 - root - INFO -> Quantizing block: 29 \n",
      "2024-06-28 17:34:44,419 - root - INFO -> Quantizing block: 30 \n",
      "2024-06-28 17:34:45,739 - root - INFO -> Quantizing block: 31 \n",
      "2024-06-28 17:34:48,477 - root - INFO -> Loaded embed\n",
      "2024-06-28 17:34:48,490 - root - INFO -> Loaded pre_norm\n",
      "2024-06-28 17:34:49,890 - root - INFO -> Loaded cls_head\n"
     ]
    }
   ],
   "source": [
    "llama_loader = ModelLoader(name=\"llama3-8b\", weight_path=\"llama3_8b_weights\", quant_type=\"nf4\")\n",
    "config = llama_loader.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_top_p(probs, p):\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "    mask = probs_sum - probs_sort > p\n",
    "    probs_sort[mask] = 0.0\n",
    "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "    next_token = torch.gather(probs_idx, -1, next_token)\n",
    "    return next_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.inference_mode()\n",
    "# def generate(\n",
    "#     prompt_tokens: List[List[int]],\n",
    "#     max_gen_len: int,\n",
    "#     temperature: float = 0.6,\n",
    "#     top_p: float = 0.9,\n",
    "#     logprobs: bool = False,\n",
    "#     echo: bool = False,\n",
    "# ) -> Tuple[List[List[int]], Optional[List[List[float]]]]:\n",
    "#     \"\"\"\n",
    "#     Generate text sequences based on provided prompts using the language generation model.\n",
    "\n",
    "#     Args:\n",
    "#         prompt_tokens (List[List[int]]): List of tokenized prompts, where each prompt is represented as a list of integers.\n",
    "#         max_gen_len (int): Maximum length of the generated text sequence.\n",
    "#         temperature (float, optional): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n",
    "#         top_p (float, optional): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n",
    "#         logprobs (bool, optional): Flag indicating whether to compute token log probabilities. Defaults to False.\n",
    "#         echo (bool, optional): Flag indicating whether to include prompt tokens in the generated output. Defaults to False.\n",
    "\n",
    "#     Returns:\n",
    "#         Tuple[List[List[int]], Optional[List[List[float]]]]: A tuple containing generated token sequences and, if logprobs is True, corresponding token log probabilities.\n",
    "\n",
    "#     Note:\n",
    "#         This method uses the provided prompts as a basis for generating text. It employs nucleus sampling to produce text with controlled randomness.\n",
    "#         If logprobs is True, token log probabilities are computed for each generated token.\n",
    "    \n",
    "#     \"\"\"\n",
    "\n",
    "#     tokenizer = Tokenizer(\"C:\\\\Users\\\\volta\\Documents\\\\GitHub\\\\z-git\\\\j-tools\\\\juju_tools\\\\models\\\\llama3\\\\tokenizer.model\")\n",
    "\n",
    "\n",
    "#     config = llama_loader.get_config()\n",
    "#     bsz = len(prompt_tokens)\n",
    "#     assert bsz <= config.bsz, (bsz, config.bsz)\n",
    "\n",
    "#     min_prompt_len = min(len(t) for t in prompt_tokens)\n",
    "#     max_prompt_len = max(len(t) for t in prompt_tokens)\n",
    "#     assert max_prompt_len <= config.max_tokens\n",
    "\n",
    "#     total_len = min(config.max_tokens, max_gen_len + max_prompt_len)\n",
    "\n",
    "#     pad_id = tokenizer.pad_id\n",
    "#     tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=\"cuda\")\n",
    "#     for k, t in enumerate(prompt_tokens):\n",
    "#         tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=\"cuda\")\n",
    "#     if logprobs:\n",
    "#         token_logprobs = torch.zeros_like(tokens, dtype=torch.float)\n",
    "\n",
    "#     prev_pos = 0\n",
    "#     eos_reached = torch.tensor([False] * bsz, device=\"cuda\")\n",
    "#     input_text_mask = tokens != pad_id\n",
    "\n",
    "#     stop_tokens = torch.tensor(list(tokenizer.stop_tokens))\n",
    "\n",
    "#     for cur_pos in range(min_prompt_len, total_len):\n",
    "#         logits = llama_loader.get_model()(tokens[:, prev_pos:cur_pos], shift=prev_pos)\n",
    "#         if temperature > 0:\n",
    "#             probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n",
    "#             next_token = sample_top_p(probs, top_p)\n",
    "#         else:\n",
    "#             next_token = torch.argmax(logits[:, -1], dim=-1)\n",
    "\n",
    "#         next_token = next_token.reshape(-1)\n",
    "#         # only replace token if prompt has already been generated\n",
    "#         next_token = torch.where(\n",
    "#             input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "#         )\n",
    "#         tokens[:, cur_pos] = next_token\n",
    "#         if logprobs:\n",
    "#             token_logprobs[:, prev_pos + 1 : cur_pos + 1] = -F.cross_entropy(\n",
    "#                 input=logits.transpose(1, 2),\n",
    "#                 target=tokens[:, prev_pos + 1 : cur_pos + 1],\n",
    "#                 reduction=\"none\",\n",
    "#                 ignore_index=pad_id,\n",
    "#             )\n",
    "#         eos_reached |= (~input_text_mask[:, cur_pos]) & (\n",
    "#             torch.isin(next_token, stop_tokens.cuda())\n",
    "#         )\n",
    "        \n",
    "#         for lool in range(tokens.shape[0]):\n",
    "#             gen_toks = tokens[lool, : cur_pos + 1].detach().cpu().numpy().tolist()\n",
    "#             logger.info(f\"{tokenizer.decode(gen_toks)}\")\n",
    "\n",
    "#         prev_pos = cur_pos\n",
    "#         if all(eos_reached):\n",
    "#             break\n",
    "\n",
    "#     if logprobs:\n",
    "#         token_logprobs = token_logprobs.tolist()\n",
    "#     out_tokens, out_logprobs = [], []\n",
    "#     for i, toks in enumerate(tokens.tolist()):\n",
    "#         # cut to max gen len\n",
    "#         start = 0 if echo else len(prompt_tokens[i])\n",
    "#         toks = toks[start : len(prompt_tokens[i]) + max_gen_len]\n",
    "#         probs = None\n",
    "#         if logprobs:\n",
    "#             probs = token_logprobs[i][start : len(prompt_tokens[i]) + max_gen_len]\n",
    "#         # cut to after eos tok if any\n",
    "#         for stop_token in tokenizer.stop_tokens:\n",
    "#             try:\n",
    "#                 eos_idx = toks.index(stop_token)\n",
    "#                 toks = toks[:eos_idx]\n",
    "#                 probs = probs[:eos_idx] if logprobs else None\n",
    "#             except ValueError:\n",
    "#                 pass\n",
    "#         out_tokens.append(toks)\n",
    "#         out_logprobs.append(probs)\n",
    "#     return (out_tokens, out_logprobs if logprobs else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama_loader.get_model().clear_cache()\n",
    "\n",
    "# tokenizer = Tokenizer(\"C:\\\\Users\\\\volta\\Documents\\\\GitHub\\\\z-git\\\\j-tools\\\\juju_tools\\\\models\\\\llama3\\\\tokenizer.model\")\n",
    "\n",
    "# cf = ChatFormat(tokenizer)\n",
    "# dialogs = [\n",
    "#         # [{\"role\": \"user\", \"content\": \"what is the recipe of mayonnaise?\"}],\n",
    "#         [\n",
    "#             {\"role\": \"user\", \"content\": \"I am going to Paris, what should I see?\"},\n",
    "#             {\n",
    "#                 \"role\": \"assistant\",\n",
    "#                 \"content\": \"\"\"\\\n",
    "# Paris, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\n",
    "\n",
    "# 1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\n",
    "# 2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\n",
    "# 3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\n",
    "\n",
    "# These are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.\"\"\",\n",
    "#             },\n",
    "#             {\"role\": \"user\", \"content\": \"What is so great about #1?\"},\n",
    "#         # ],\n",
    "#         # [\n",
    "#         #     {\"role\": \"system\", \"content\": \"Always answer with Haiku\"},\n",
    "#         #     {\"role\": \"user\", \"content\": \"I am going to Paris, what should I see?\"},\n",
    "#         # ],\n",
    "#         # [\n",
    "#         #     {\n",
    "#         #         \"role\": \"system\",\n",
    "#         #         \"content\": \"Always answer with emojis\",\n",
    "#         #     },\n",
    "#         #     {\"role\": \"user\", \"content\": \"How to go from Beijing to NY?\"},\n",
    "#         ],\n",
    "#     ]\n",
    "\n",
    "# prompt_tokens = [cf.encode_dialog_prompt(dialog) for dialog in dialogs]\n",
    "\n",
    "# generate(prompt_tokens=prompt_tokens, temperature=0, max_gen_len=120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_tok = 0\n",
    "\n",
    "@torch.inference_mode()\n",
    "def run_local_model(message: list[dict]) -> None:\n",
    "    global prev_tok\n",
    "    with torch.no_grad():\n",
    "\n",
    "        gen_len = 120\n",
    "\n",
    "        tok = Tokenizer(\"C:\\\\Users\\\\volta\\Documents\\\\GitHub\\\\z-git\\\\j-tools\\\\juju_tools\\\\models\\\\llama3\\\\tokenizer.model\")\n",
    "\n",
    "        cf = ChatFormat(tok)\n",
    "\n",
    "        # y = \"The meaning of life is to\"  # torch.randint(10, (2, 10)).cuda()\n",
    "        # y = [tok.encode(s=y, bos=True, eos=False)]\n",
    "\n",
    "        logger.info(f\"{message}\")\n",
    "        \n",
    "        y = [cf.encode_dialog_prompt(message)]\n",
    "\n",
    "        tokens = torch.full((1, len(y[0]) + gen_len), fill_value=config.pad_tok_idx,\n",
    "                            dtype=torch.long, device=config.device)\n",
    "\n",
    "        for idx, t in enumerate(y):\n",
    "            tokens[idx, :len(t)] = torch.tensor(t, dtype=torch.long, device=config.device)\n",
    "\n",
    "        text_mask = tokens != config.pad_tok_idx\n",
    "\n",
    "        stops = torch.tensor(list(tok.stop_tokens))\n",
    "\n",
    "        \n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        for curr_tok in range(len(y[0]) + prev_tok, gen_len + len(y[0]) + prev_tok):\n",
    "\n",
    "            logger.info(f\"Current Token Being Generated {curr_tok} | {tokens[..., prev_tok:curr_tok]}\")\n",
    "\n",
    "            logits = llama_loader.get_model()(input=tokens[:, prev_tok:curr_tok], shift=prev_tok)\n",
    "\n",
    "            probs = torch.softmax(logits[:, -1] / .6, dim=-1)\n",
    "            next_tok = sample_top_p(probs, .9)  # torch.argmax(logits[:, -1], dim=-1).reshape(-1) #\n",
    "\n",
    "            next_tok = torch.where(text_mask[:, curr_tok], tokens[:, curr_tok], next_tok)\n",
    "\n",
    "            tokens[:, curr_tok] = next_tok\n",
    "\n",
    "            logger.info(f\"TOKEN: {next_tok}\")\n",
    "\n",
    "            gen_toks = tokens[0, : curr_tok + 1].detach().cpu().numpy().tolist()\n",
    "\n",
    "            logger.info(f\"{tok.decode(gen_toks)}\")\n",
    "            prev_tok = curr_tok\n",
    "\n",
    "            if torch.isin(next_tok.cpu(), stops):\n",
    "                break\n",
    "\n",
    "        logger.info(f\"Computation time: {time.time() - start} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main(**kwargs) -> Optional[Tensor]:\n",
    "    return run_local_model(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-28 17:35:18,726 - root - INFO -> Starting task... \n",
      "2024-06-28 17:35:18,983 - juju_tools.models.llama3.llama_tokenizer - INFO -> Reloaded tiktoken model from C:\\Users\\volta\\Documents\\GitHub\\z-git\\j-tools\\juju_tools\\models\\llama3\\tokenizer.model\n",
      "2024-06-28 17:35:18,984 - juju_tools.models.llama3.llama_tokenizer - INFO -> #words: 128256 - BOS ID: 128000 - EOS ID: 128001\n",
      "2024-06-28 17:35:18,985 - root - INFO -> [{'role': 'user', 'content': \"It's on World War II mate.\"}]\n",
      "2024-06-28 17:35:19,008 - root - INFO -> Current Token Being Generated 18 | tensor([[128000, 128006,    882, 128007,    271,   2181,    596,    389,   4435,\n",
      "           5111,   8105,  30276,     13, 128009, 128006,  78191, 128007,    271]],\n",
      "       device='cuda:0')\n",
      "2024-06-28 17:35:23,140 - root - INFO -> TOKEN: tensor([[5028]], device='cuda:0')\n",
      "2024-06-28 17:35:23,142 - root - INFO -> <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "It's on World War II mate.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Bl\n",
      "2024-06-28 17:35:23,146 - root - INFO -> Current Token Being Generated 19 | tensor([[5028]], device='cuda:0')\n",
      "2024-06-28 17:35:24,111 - root - INFO -> TOKEN: tensor([[547]], device='cuda:0')\n",
      "2024-06-28 17:35:24,112 - root - INFO -> <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "It's on World War II mate.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Blime\n",
      "2024-06-28 17:35:24,112 - root - INFO -> Current Token Being Generated 20 | tensor([[547]], device='cuda:0')\n",
      "2024-06-28 17:35:25,090 - root - INFO -> TOKEN: tensor([[88]], device='cuda:0')\n",
      "2024-06-28 17:35:25,092 - root - INFO -> <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "It's on World War II mate.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Blimey\n",
      "2024-06-28 17:35:25,092 - root - INFO -> Current Token Being Generated 21 | tensor([[88]], device='cuda:0')\n",
      "2024-06-28 17:35:26,130 - root - INFO -> TOKEN: tensor([[0]], device='cuda:0')\n",
      "2024-06-28 17:35:26,130 - root - INFO -> <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "It's on World War II mate.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Blimey!\n",
      "2024-06-28 17:35:26,136 - root - INFO -> Current Token Being Generated 22 | tensor([[0]], device='cuda:0')\n",
      "2024-06-28 17:35:27,169 - root - INFO -> TOKEN: tensor([[1102]], device='cuda:0')\n",
      "2024-06-28 17:35:27,171 - root - INFO -> <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "It's on World War II mate.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Blimey! It\n",
      "2024-06-28 17:35:27,172 - root - INFO -> Current Token Being Generated 23 | tensor([[1102]], device='cuda:0')\n",
      "2024-06-28 17:35:28,214 - root - INFO -> TOKEN: tensor([[596]], device='cuda:0')\n",
      "2024-06-28 17:35:28,215 - root - INFO -> <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "It's on World War II mate.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Blimey! It's\n",
      "2024-06-28 17:35:28,216 - root - INFO -> Current Token Being Generated 24 | tensor([[596]], device='cuda:0')\n",
      "2024-06-28 17:35:29,221 - root - INFO -> TOKEN: tensor([[264]], device='cuda:0')\n",
      "2024-06-28 17:35:29,221 - root - INFO -> <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "It's on World War II mate.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Blimey! It's a\n",
      "2024-06-28 17:35:29,224 - root - INFO -> Current Token Being Generated 25 | tensor([[264]], device='cuda:0')\n",
      "2024-06-28 17:35:30,208 - root - INFO -> TOKEN: tensor([[1314]], device='cuda:0')\n",
      "2024-06-28 17:35:30,209 - root - INFO -> <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "It's on World War II mate.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Blimey! It's a right\n",
      "2024-06-28 17:35:30,211 - root - INFO -> Current Token Being Generated 26 | tensor([[1314]], device='cuda:0')\n",
      "2024-06-28 17:35:31,176 - root - INFO -> TOKEN: tensor([[8492]], device='cuda:0')\n",
      "2024-06-28 17:35:31,177 - root - INFO -> <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "It's on World War II mate.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Blimey! It's a right ol\n",
      "2024-06-28 17:35:31,179 - root - INFO -> Current Token Being Generated 27 | tensor([[8492]], device='cuda:0')\n",
      "2024-06-28 17:35:32,199 - root - INFO -> TOKEN: tensor([[6]], device='cuda:0')\n",
      "2024-06-28 17:35:32,201 - root - INFO -> <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "It's on World War II mate.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Blimey! It's a right ol'\n",
      "2024-06-28 17:35:32,202 - root - INFO -> Current Token Being Generated 28 | tensor([[6]], device='cuda:0')\n",
      "2024-06-28 17:35:33,220 - root - INFO -> TOKEN: tensor([[4208]], device='cuda:0')\n",
      "2024-06-28 17:35:33,220 - root - INFO -> <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "It's on World War II mate.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Blimey! It's a right ol' war\n",
      "2024-06-28 17:35:33,226 - root - INFO -> Current Token Being Generated 29 | tensor([[4208]], device='cuda:0')\n",
      "2024-06-28 17:35:34,180 - root - INFO -> TOKEN: tensor([[11]], device='cuda:0')\n",
      "2024-06-28 17:35:34,181 - root - INFO -> <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "It's on World War II mate.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Blimey! It's a right ol' war,\n",
      "2024-06-28 17:35:34,182 - root - INFO -> Current Token Being Generated 30 | tensor([[11]], device='cuda:0')\n",
      "2024-06-28 17:35:35,099 - root - INFO -> TOKEN: tensor([[6301]], device='cuda:0')\n",
      "2024-06-28 17:35:35,099 - root - INFO -> <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "It's on World War II mate.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Blimey! It's a right ol' war, inn\n",
      "2024-06-28 17:35:35,100 - root - INFO -> Current Token Being Generated 31 | tensor([[6301]], device='cuda:0')\n",
      "2024-06-28 17:35:36,015 - root - INFO -> TOKEN: tensor([[275]], device='cuda:0')\n",
      "2024-06-28 17:35:36,016 - root - INFO -> <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "It's on World War II mate.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Blimey! It's a right ol' war, innit\n",
      "2024-06-28 17:35:36,016 - root - INFO -> Current Token Being Generated 32 | tensor([[275]], device='cuda:0')\n",
      "2024-06-28 17:35:36,939 - root - INFO -> TOKEN: tensor([[30]], device='cuda:0')\n",
      "2024-06-28 17:35:36,940 - root - INFO -> <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "It's on World War II mate.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Blimey! It's a right ol' war, innit?\n",
      "2024-06-28 17:35:36,941 - root - INFO -> Current Token Being Generated 33 | tensor([[30]], device='cuda:0')\n",
      "2024-06-28 17:35:37,851 - root - INFO -> TOKEN: tensor([[128009]], device='cuda:0')\n",
      "2024-06-28 17:35:37,852 - root - INFO -> <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "It's on World War II mate.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Blimey! It's a right ol' war, innit?<|eot_id|>\n",
      "2024-06-28 17:35:37,853 - root - INFO -> Computation time: 18.866268157958984 seconds\n",
      "2024-06-28 17:35:37,879 - root - INFO -> ...Finished\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Starting task... \")\n",
    "    \n",
    "    # llama_loader.get_model().clear_cache()\n",
    "\n",
    "    message = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a smart chatbot.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Write me a World War II essay, please, thank you.\"},\n",
    "        ]\n",
    "\n",
    "    main(message=message)\n",
    "    # get_hf_weights()\n",
    "    logger.info(\"...Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from juju_tools.models.llama3.get_llama3_hf import save_model_weights\n",
    "# import os\n",
    "# import time\n",
    "\n",
    "# import torch\n",
    "\n",
    "# from juju_tools.utils import *\n",
    "\n",
    "# from juju_tools.models.loading_utils import ModelLoader\n",
    "# from juju_tools.models.llama3 import Tokenizer, ChatFormat\n",
    "\n",
    "# def get_hf_weights() -> None:\n",
    "#     logger.info(f\"{os.getcwd()}\")\n",
    "#     hf_path = input(\"Enter the local hugging face path: \")\n",
    "#     save_model_weights(hf_path, str(os.getcwd()) + \"\\\\\")\n",
    "    \n",
    "# get_hf_weights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
